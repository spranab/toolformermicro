% =========================================================================
% Shared bibliography for ToolFormerMicro and ContextCache papers
% =========================================================================

% --- Tool-augmented LLMs ---

@article{schick2024toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{patil2023gorilla,
  title={Gorilla: Large Language Model Connected with Massive APIs},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{qin2024toolllm,
  title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{liu2024xlam,
  title={xLAM: A Family of Large Action Models to Empower AI Agent Systems},
  author={Liu, Jianguo and Zhu, Huan and Liu, Bo and others},
  journal={arXiv preprint arXiv:2409.03215},
  year={2024}
}

% --- Context compression ---

@article{mu2024learning,
  title={Learning to Compress Prompts with Gist Tokens},
  author={Mu, Jesse and Li, Xiang Lisa and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chevalier2023autocompressors,
  title={Adapting Language Models to Compress Contexts},
  author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
  journal={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{ge2024incontext,
  title={In-context Autoencoder for Context Compression in a Large Language Model},
  author={Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{jiang2023longllmlingua,
  title={LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression},
  author={Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongkuan and Lin, Chi-Yan and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.06839},
  year={2023}
}

% --- Cross-attention architectures ---

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General Perception with Iterative Attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  booktitle={International Conference on Machine Learning},
  pages={4651--4664},
  year={2021}
}

@article{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}

@inproceedings{li2023blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={19730--19742},
  year={2023}
}

% --- KV cache and serving ---

@inproceedings{kwon2023vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{zheng2024sglang,
  title={SGLang: Efficient Execution of Structured Language Model Programs},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Huang, Jeff and Sun, Chuyue and Yu, Cody Hao and Cao, Shiyi and Kober, Christos and Shi, Yinmin and Sheng, Ying and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2024}
}

@inproceedings{gim2024prompt,
  title={Prompt Cache: Modular Attention Reuse for Low-Latency Inference},
  author={Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Kalbarczyk, Zbigniew and Iyer, Ravishankar K},
  booktitle={Proceedings of Machine Learning and Systems},
  year={2024}
}

@inproceedings{yao2024cacheblend,
  title={CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and others},
  booktitle={Proceedings of the Nineteenth European Conference on Computer Systems},
  year={2025}
}

% --- Positional encoding ---

@article{kazemnejad2024nope,
  title={The Impact of Positional Encoding on Length Generalization in Transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{press2022alibi,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={International Conference on Learning Representations},
  year={2022}
}

@article{su2024roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024}
}

% --- Foundation models ---

@article{qwen2024qwen25,
  title={Qwen2.5 Technical Report},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{qwen2025qwen3,
  title={Qwen3 Technical Report},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

% --- Training techniques ---

@article{hu2022lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={International Conference on Learning Representations},
  year={2022}
}

@article{dettmers2024qlora,
  title={QLoRA: Efficient Finetuning of Quantized Language Models},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{oord2018representation,
  title={Representation Learning with Contrastive Predictive Coding},
  author={van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{dettmers2022gptint8,
  title={GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}

@article{chen2016training,
  title={Training Deep Nets with Sublinear Memory Cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

% --- Companion papers (cross-references) ---

@techreport{toolformermicro2026,
  title={ToolFormerMicro: Composable Tool Schema Compression via Gated Cross-Attention},
  author={Sarkar, Pranab},
  year={2026},
  institution={Zenodo},
  note={Companion paper}
}

@techreport{contextcache2026,
  title={ContextCache: Persistent {KV} Cache with Content-Hash Addressing for Zero-Degradation Tool Schema Caching},
  author={Sarkar, Pranab},
  year={2026},
  institution={Zenodo},
  note={Companion paper}
}
