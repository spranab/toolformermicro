\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{newtxtext,newtxmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{algorithm2e}
\usepackage[numbers]{natbib}
\usepackage{microtype}
\usepackage{enumitem}

\graphicspath{{../../figures/tool_former/}}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  citecolor=green!50!black,
  urlcolor=blue!70!black,
}

\title{ToolFormerMicro: Composable Tool Schema Compression\\via Gated Cross-Attention}

\author{
  Pranab Sarkar\\
  ORCID: 0009-0009-8683-1481
}

\date{February 2026}

\begin{document}
\maketitle

% ===========================================================================
\begin{abstract}
Tool-augmented large language models (LLMs) include full JSON tool schemas in every prompt, consuming thousands of tokens on context that rarely changes between requests. We present \textbf{ToolFormerMicro}, a small encoder-decoder model (${\sim}428$M parameters, ${<}1$\,GB) that compresses each tool schema independently into $K{=}8$ fixed-size gist vectors via Perceiver-style cross-attention pooling. A query decoder consumes these gist vectors through \emph{gated cross-attention}---where a learned $\tanh$ gate initialized to zero prevents randomly-initialized cross-attention layers from corrupting pre-trained decoder states. Tools are encoded independently and their gists are simply concatenated, enabling true composability: adding or removing a tool requires re-encoding only that tool (${\sim}40$\,ms) with no effect on other tools' cached representations. We evaluate on three tool-calling benchmarks (seen, held-out, and unseen tools) and demonstrate: (1)~Tool Selection Accuracy of 0.818 with zero false positives across all splits; (2)~perfect order independence ($\Delta$TSA$\,{=}\,0.000$); (3)~constant accuracy from 5 to 200 tools with sub-linear encoding cost; and (4)~bit-identical gist stability under single-tool hot-swap. Each tool's gist cache is 14\,KB, enabling deployment on resource-constrained devices.
\end{abstract}

% ===========================================================================
\section{Introduction}
\label{sec:intro}

Tool-augmented LLMs have become central to agentic AI systems, enabling models to call APIs, query databases, and interact with external services~\citep{schick2024toolformer,patil2023gorilla,qin2024toolllm}. In practice, a tool-calling prompt includes JSON schemas describing each available tool's name, parameters, and descriptions. For a system with $N$ tools averaging $T$ tokens each, this represents $O(NT)$ prefix tokens that are reprocessed from scratch on every user request---even though the tool catalog changes far less frequently than user queries.

This redundancy has two costs. First, \textbf{latency}: prefilling thousands of tool-schema tokens dominates time-to-first-token (TTFT), with measurements showing 787\,ms for 20 tools on Qwen3-8B versus 114\,ms for the user query alone~\citep[companion paper]{contextcache2026}. Second, \textbf{composability}: existing solutions like prefix caching~\citep{kwon2023vllm,zheng2024sglang} cache the entire tool set as a single prefix, so adding or removing one tool invalidates the entire cache.

We propose \textbf{ToolFormerMicro}, a small encoder-decoder architecture that addresses both problems simultaneously. Each tool schema is encoded independently into $K{=}8$ gist vectors (14\,KB per tool). The decoder attends to the concatenated gist vectors of all active tools via cross-attention. This design yields three composability properties that we verify empirically:

\begin{enumerate}[leftmargin=*,topsep=2pt,itemsep=1pt]
\item \textbf{Order independence}: Shuffling the order of tool gists produces identical outputs ($\Delta$TSA$\,{=}\,0.000$).
\item \textbf{Sub-linear scaling}: Tool Selection Accuracy remains constant (0.800) from 5 to 200 tools, with per-tool encoding cost decreasing from 5.35 to 0.35\,ms.
\item \textbf{Hot-swap stability}: Replacing one tool's schema leaves all other tools' gist vectors bit-identical and does not affect routing accuracy.
\end{enumerate}

A key technical contribution is \emph{gated cross-attention}. When grafting randomly-initialized cross-attention layers onto a pre-trained decoder, the random projections corrupt hidden states, causing training to plateau. Our solution is a learnable scalar gate $g_l$ per decoder layer, passed through $\tanh$ and initialized to zero. This makes cross-attention a no-op at initialization ($\tanh(0)=0$), allowing the model to smoothly learn when and how much to incorporate tool information.

ToolFormerMicro is initialized from Qwen2.5-0.5B~\citep{qwen2024qwen25} (${\sim}428$M parameters) and trained in three stages: schema auto-encoding (3K steps), contrastive gist discrimination (2K steps), and end-to-end tool calling (3 epochs). On 200-example test sets across three splits, the model achieves TSA=0.818, Parameter F1=0.759, Value Recall=0.917, and zero false positives---in a model 20$\times$ smaller than the 8B baseline.

% ===========================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Tool-augmented LLMs.}
Toolformer~\citep{schick2024toolformer} pioneered self-supervised tool use via API call insertion. Gorilla~\citep{patil2023gorilla} and ToolLLM~\citep{qin2024toolllm} scale to thousands of APIs with retrieval-augmented generation. xLAM~\citep{liu2024xlam} provides large action models with unified tool-calling formats. All these approaches include full tool schemas in the prompt, creating the redundancy we address.

\paragraph{Context compression.}
Gisting~\citep{mu2024learning} learns soft ``gist tokens'' that compress prompt segments within a decoder-only model. AutoCompressors~\citep{chevalier2023autocompressors} use summary vectors for long-range context. ICAE~\citep{ge2024incontext} trains an in-context autoencoder for lossy compression. LongLLMLingua~\citep{jiang2023longllmlingua} prunes prompts via perplexity-guided selection. Unlike these approaches, which compress within a single model's embedding space, ToolFormerMicro uses a separate encoder to produce composable representations that can be cached and recombined.

\paragraph{Cross-attention architectures.}
Perceiver~\citep{jaegle2021perceiver} uses cross-attention with learned queries to distill arbitrary inputs into fixed-size representations. Flamingo~\citep{alayrac2022flamingo} interleaves gated cross-attention layers within a frozen language model for visual conditioning. Q-Former in BLIP-2~\citep{li2023blip2} learns queries that extract visual features for language model consumption. Our GistPooling and gated decoder cross-attention draw directly from this line of work, adapted for tool schema compression.

\paragraph{KV cache optimization.}
PagedAttention~\citep{kwon2023vllm} manages KV cache memory efficiently for serving. SGLang~\citep{zheng2024sglang} uses RadixAttention for prefix sharing. Prompt Cache~\citep{gim2024prompt} enables modular KV reuse via prompt markup. These systems cache raw KV states, which scale linearly with context length. Our gist approach compresses each tool to a fixed 14\,KB regardless of schema length.

% ===========================================================================
\section{Method}
\label{sec:method}

\subsection{Architecture Overview}

ToolFormerMicro is an encoder-decoder model initialized from Qwen2.5-0.5B~\citep{qwen2024qwen25} (24 layers, 896 hidden dimension, 14 attention heads with 2 KV heads). The architecture has three components:

\begin{itemize}[leftmargin=*,topsep=2pt,itemsep=1pt]
\item \textbf{Tool Encoder}: 6 transformer layers (from Qwen layers 0--5) that process each tool schema independently. Input: tokenized schema (up to 256 tokens). Output: contextualized token representations.
\item \textbf{Gist Pooling}: A Perceiver-style cross-attention bottleneck with $K{=}8$ learnable query vectors. Input: encoder output. Output: $K$ gist vectors, each of dimension $d{=}896$.
\item \textbf{Query Decoder}: 12 transformer layers (from Qwen layers 6--17) with interleaved \emph{gated cross-attention}. Input: user query tokens + cross-attention to tool gist memory. Output: autoregressive response with tool calls.
\end{itemize}

Total parameter count: ${\sim}428$M (encoder ${\sim}98$M, decoder ${\sim}308$M, new cross-attention and gist pooling ${\sim}22$M). The model occupies $<1$\,GB at fp16.

\subsection{Gated Cross-Attention}
\label{sec:gated}

Each decoder layer follows the structure:
\begin{align}
  \mathbf{h}' &= \mathbf{h} + \text{SelfAttn}(\text{LN}(\mathbf{h})) \label{eq:self-attn} \\
  \mathbf{h}'' &= \mathbf{h}' + \tanh(g_l) \cdot \text{CrossAttn}(\text{LN}(\mathbf{h}'), \mathbf{M}) \label{eq:cross-attn} \\
  \mathbf{h}''' &= \mathbf{h}'' + \text{FFN}(\text{LN}(\mathbf{h}'')) \label{eq:ffn}
\end{align}
where $g_l$ is a learnable scalar per layer (initialized to 0), $\text{LN}$ is RMSNorm, and $\mathbf{M} = [\mathbf{m}_1; \mathbf{m}_2; \ldots; \mathbf{m}_N]$ is the concatenated gist memory from $N$ tools, each contributing $K{=}8$ vectors.

\paragraph{Why gating is necessary.} When adding randomly-initialized cross-attention layers to a pre-trained decoder, the random key/value projections produce essentially noise-like outputs. These corrupt the pre-trained hidden states from the very first training step. We observed that without gating, training loss plateaus at ${\sim}15$ for the first 2,500 steps before slowly recovering to ${\sim}5.0$. With $\tanh$ gating initialized to zero, training converges smoothly, reaching loss ${\sim}3.7$ at the same training budget.

This mechanism is inspired by Flamingo~\citep{alayrac2022flamingo}, which uses a similar $\tanh$ gate when injecting visual information into a frozen language model. Our contribution is applying it to tool schema conditioning and demonstrating its necessity for stable pre-trained weight transfer in a fully fine-tuned (not frozen) decoder.

\subsection{Gist Pooling}

The Gist Pooling module uses $K{=}8$ learnable query vectors $\mathbf{Q} \in \mathbb{R}^{K \times d}$ that attend to the encoder output $\mathbf{E} \in \mathbb{R}^{T \times d}$ via cross-attention:
\begin{equation}
  \mathbf{m}_i = \text{CrossAttn}(\text{LN}(\mathbf{Q}), \mathbf{E}_i) \in \mathbb{R}^{K \times d}
  \label{eq:gist-pool}
\end{equation}
where $\mathbf{E}_i$ is the encoder output for tool $i$. Each tool produces $K$ gist vectors independently of all other tools. The gist vectors are then concatenated to form the tool memory $\mathbf{M} = [\mathbf{m}_1; \ldots; \mathbf{m}_N] \in \mathbb{R}^{NK \times d}$.

\paragraph{Composability.} Because each tool is encoded and pooled independently, the encoder never sees other tools' tokens. This means: (a)~encoding tool $i$ is $O(T_i)$ regardless of $N$; (b)~adding tool $j$ requires encoding only $j$; (c)~the cached gist for tool $i$ is invalidated only when tool $i$'s schema changes.

\subsection{Training Curriculum}
\label{sec:training}

We train in three stages with increasing task complexity:

\paragraph{Stage 1: Schema Auto-Encoding (3K steps).}
The encoder-decoder learns to reconstruct tool schemas from their gist representations. Given a schema $s_i$, we encode it to gists $\mathbf{m}_i$, then train the decoder to generate $s_i$ auto-regressively from $\mathbf{m}_i$ alone. This teaches the encoder to produce informative gist vectors and forces the gated cross-attention to open (gates must become nonzero for reconstruction). Learning rate: $2 \times 10^{-4}$, batch size: 8.

\paragraph{Stage 1.5: Contrastive Gist Discrimination (2K steps).}
We add an InfoNCE~\citep{oord2018representation} contrastive loss to ensure gist vectors are discriminative across tools. For each (query, target-tool) pair, the mean-pooled query embedding and mean-pooled tool gist are pushed together while 7 hard-negative tools are pushed apart. Temperature $\tau{=}0.07$. An auxiliary schema AE loss ($\lambda{=}0.1$) prevents catastrophic forgetting. Learning rate: $1 \times 10^{-4}$.

\paragraph{Stage 2: End-to-End Tool Calling (3 epochs, ${\sim}$18.8K steps).}
Full training on 100K examples. Each example includes 20 tool schemas, a user query, and a target response (either a tool call or a text response). All parameters are unfrozen. Auxiliary AE ($\lambda{=}0.1$, applied to 5\% of batches) and contrastive ($\lambda{=}0.1$) losses regularize the encoder. Learning rate: $1 \times 10^{-4}$, effective batch size: 16 (4 $\times$ 4 gradient accumulation), gradient checkpointing for 24\,GB GPU memory.

\subsection{Composable Inference}

At inference time:
\begin{enumerate}[leftmargin=*,topsep=2pt,itemsep=1pt]
\item \textbf{Encode} each tool schema independently: $\mathbf{m}_i = \text{GistPool}(\text{Encoder}(s_i))$. Cache $\mathbf{m}_i$ to disk/memory (14\,KB per tool at fp16).
\item \textbf{Compose} tool memory: $\mathbf{M} = [\mathbf{m}_1; \ldots; \mathbf{m}_N]$.
\item \textbf{Decode} the user query autoregressively with cross-attention to $\mathbf{M}$.
\end{enumerate}
When a tool is added, only that tool's schema is encoded (${\sim}40$\,ms). When a tool is removed, its gist vectors are simply dropped from $\mathbf{M}$. The remaining tools' gists are untouched.

% ===========================================================================
\section{Experimental Setup}
\label{sec:experiments}

\paragraph{Dataset.}
We combine tool-calling data from xLAM~\citep{liu2024xlam}, Hermes, and Glaive open-source datasets, yielding 100K training examples with 3,200 unique tool schemas. Each example includes 20 tool schemas, a user query, and a gold response. We construct three test splits of $N{=}200$ examples each: \textbf{test\_seen} (tools from training), \textbf{test\_held\_out} (training tools, novel query patterns), and \textbf{test\_unseen} (entirely novel tools). Ground-truth tool-call labels are derived from gold response content (\texttt{<functioncall>} tag presence), not metadata fields, to avoid a mislabeling issue we identified where 38.6\% of originally-labeled ``non-tool-call'' examples actually contained tool calls.

\paragraph{Metrics.}
\textbf{TSA} (Tool Selection Accuracy): correct tool selected among tool-call queries. \textbf{PF1} (Parameter F1): F1 on parameter name-value pairs. \textbf{VR} (Value Recall): fraction of parameter values correctly extracted. \textbf{EM} (Exact Match): full response match. \textbf{FPR/FNR}: false positive/negative rates for tool-call routing.

\paragraph{Baselines.}
(1)~ContextCache~\citep{contextcache2026}: Qwen3-8B~\citep{qwen2025qwen3} with group KV caching (8B model, full tool schemas in context). (2)~Tool Gisting K=8: soft gist tokens trained within Qwen3-8B following~\citet{mu2024learning}.

\paragraph{Hardware.}
Single NVIDIA RTX 3090 Ti (24\,GB). Training: ${\sim}15$ hours total across all three stages. Inference: fp16.

% ===========================================================================
\section{Results}
\label{sec:results}

\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Method comparison. ToolFormerMicro V1 achieves 0.818 TSA with zero false positives in a model 20$\times$ smaller than the 8B baseline. Results are identical across all three test splits.}
\label{tab:main-results}
\small
\begin{tabular}{lccccc}
\toprule
Method & TSA$\uparrow$ & PF1$\uparrow$ & VR$\uparrow$ & EM$\uparrow$ & FPR$\downarrow$ \\
\midrule
\textbf{ToolFormerMicro V1} & \textbf{0.818} & 0.759 & 0.917 & 0.580 & \textbf{0.000} \\
ToolFormerMicro V2 (gated) & 0.784 & \textbf{0.792} & \textbf{0.942} & 0.580 & 0.000 \\
ContextCache (8B) & 0.850 & 0.735 & --- & 0.600 & 0.000 \\
Tool Gisting K=8 & 0.714 & --- & --- & --- & 0.302 \\
\bottomrule
\end{tabular}
\end{table}

\Cref{tab:main-results} compares ToolFormerMicro against baselines. Key findings:

\begin{itemize}[leftmargin=*,topsep=2pt,itemsep=1pt]
\item \textbf{Zero false positives}: Both V1 and V2 never hallucinate tool calls when none is needed. Tool Gisting K=8 produces spurious tool calls 30.2\% of the time.
\item \textbf{Generalization}: Metrics are \emph{identical} across test\_seen, test\_held\_out, and test\_unseen splits. The model generalizes perfectly to novel tools not seen during training.
\item \textbf{Value Recall gap}: VR=0.917 versus EM=0.580 indicates the model captures parameter semantics but uses its own naming conventions for parameter keys (see \Cref{sec:discussion}).
\item \textbf{Size--quality tradeoff}: The 8B ContextCache achieves slightly higher TSA (0.850) and EM (0.600), but ToolFormerMicro is 20$\times$ smaller and provides true per-tool composability.
\end{itemize}

\subsection{Composability Experiments}

\begin{table}[t]
\centering
\caption{Composability verification. ToolFormerMicro gist vectors are fully composable: order-independent, scale to 200 tools with constant TSA, and support single-tool hot-swap with bit-identical stability.}
\label{tab:composability}
\small
\begin{tabular}{lcc}
\toprule
Experiment & Metric & Result \\
\midrule
Order Independence & TSA delta & 0.000 (+0.000) \\
Scaling (5$\to$200 tools) & TSA range & [0.800, 0.800] \\
Cache Hot-Swap & TSA delta & 0.000 (re-encode: 40ms) \\
  & Other tools & bit-identical \\
\bottomrule
\end{tabular}
\end{table}

We design three experiments to verify composability properties (\Cref{tab:composability}):

\paragraph{Experiment 1: Order Independence.}
We run 50 examples with tools in their standard catalog order (TSA=0.860) and again with tools randomly shuffled (TSA=0.860). The delta is exactly zero, confirming that the decoder's cross-attention is permutation-invariant over gist vectors.

\paragraph{Experiment 2: Scaling.}
We vary the number of tools from 5 to 200 (\Cref{fig:scaling}). TSA remains constant at 0.800 across all tool counts. Per-tool encoding cost decreases from 5.35\,ms (5 tools) to 0.35\,ms (200 tools) due to GPU batching efficiency. The cross-attention over $NK$ gist tokens ($K{=}8$) at $N{=}200$ processes 1,600 tokens---still well within the model's capacity.

\paragraph{Experiment 3: Hot-Swap Cache Invalidation.}
We cache gist vectors for 20 tools, then replace one tool's schema with a modified version. After re-encoding only the modified tool (39.8\,ms), we verify: (a)~all 19 other tools' gist vectors are bit-identical to before the swap; (b)~TSA is unchanged (0.800 before and after). This confirms that tool gists are truly independent.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig1_scaling_curves.pdf}
  \caption{Scaling from 5 to 200 tools. TSA remains constant at 0.800 regardless of tool count. Per-tool encoding cost decreases sub-linearly due to GPU batching.}
  \label{fig:scaling}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig2_composability_suite.pdf}
  \caption{Composability suite: order independence (left), scaling curves (center), and hot-swap stability (right).}
  \label{fig:composability}
\end{figure}

\subsection{Latency Analysis}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig4_latency_analysis.pdf}
  \caption{Latency breakdown. Tool encoding averages 18\,ms per batch. Generation takes 2.7--3.3\,s (acceptable for a $<$1\,GB model with greedy decoding).}
  \label{fig:latency}
\end{figure}

\Cref{fig:latency} shows the latency breakdown. Tool encoding averages ${\sim}18$\,ms per batch (amortized to 0.35--5.35\,ms per tool depending on batch size). Generation takes 2.7--3.3\,s with greedy decoding. The gist cache per tool is 14\,KB ($K{=}8$ vectors $\times$ $d{=}896$ dimensions $\times$ 2 bytes fp16), meaning 20 tools require only 280\,KB of cache versus ${\sim}400$\,MB for full 8B KV cache.

\subsection{Gated vs.\ Ungated Cross-Attention}

We compare V1 (trained without gating but eventually converged) and V2 (with $\tanh$ gating from initialization):

\begin{itemize}[leftmargin=*,topsep=2pt,itemsep=1pt]
\item \textbf{V1}: TSA=0.818, PF1=0.759, VR=0.917. Better at tool routing.
\item \textbf{V2}: TSA=0.784, PF1=0.792, VR=0.942. Better at parameter extraction.
\item \textbf{Convergence}: V2 converges smoothly (loss ${\sim}3.7$); V1 plateaus early then slowly recovers (loss ${\sim}5.0$).
\end{itemize}

We select V1 as the primary model because TSA (correct tool routing) is the key metric for composable deployment. However, V2's superior parameter extraction suggests that the gating mechanism helps the model focus on fine-grained schema details.

% ===========================================================================
\section{Discussion}
\label{sec:discussion}

\paragraph{Parameter naming gap.}
The gap between VR=0.917 and EM=0.580 reveals that ToolFormerMicro captures tool semantics through gist compression but loses exact parameter key names. The model correctly identifies parameter values 91.7\% of the time but uses its own naming conventions. This is addressable via post-processing (schema-aware key remapping) and reflects a fundamental property of lossy compression: semantic content is preserved while surface forms are not.

\paragraph{False negative rate.}
The 18.2\% FNR means the model defaults to text responses for queries that should invoke tools. This likely stems from the training data distribution (approximately 56\% non-tool vs.\ 44\% tool-call examples) biasing the model toward conservative routing. Potential mitigations include balanced sampling, routing-specific loss weighting, or a separate lightweight classifier.

\paragraph{Comparison to full-size models.}
ContextCache (Qwen3-8B) achieves higher TSA (0.850 vs.\ 0.818) with full tool schemas in context. This 3.2 percentage point gap is the cost of 20$\times$ model compression and gist-based lossy representation. For deployments where model size matters (edge devices, cost-sensitive serving), ToolFormerMicro is the better choice. For maximum accuracy with unlimited GPU, the 8B model with KV caching remains preferable.

% ===========================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented ToolFormerMicro, a ${\sim}428$M parameter encoder-decoder that compresses tool schemas into composable gist vectors via gated cross-attention. The three verified composability properties---order independence, sub-linear scaling to 200 tools, and bit-identical hot-swap---demonstrate that tool representations can be truly modular. At 14\,KB per tool and $<$1\,GB model size, ToolFormerMicro enables tool-calling on resource-constrained devices.

The key technical insight is that $\tanh$-gated cross-attention, initialized to zero, enables stable fine-tuning when grafting new cross-attention layers onto pre-trained decoder weights. This mechanism may be broadly useful for any setting where pre-trained models need to be conditioned on new modalities.

Future work includes scaling to thousands of tools (potentially with hierarchical gist pooling), multi-turn conversation support, and combining ToolFormerMicro as a lightweight router with full-size models for execution.

\paragraph{Reproducibility.} Code, data, and trained checkpoints are available at \url{https://github.com/spranab/toolformermicro}.

% ===========================================================================
\bibliographystyle{plainnat}
\bibliography{../references}

% ===========================================================================
\appendix
\section{Training Hyperparameters}
\label{app:hyperparams}

\begin{table}[h]
\centering
\caption{Complete training hyperparameters for all stages.}
\label{tab:hyperparams}
\small
\begin{tabular}{llccc}
\toprule
Parameter & & Stage 1 & Stage 1.5 & Stage 2 \\
\midrule
Task & & Schema AE & Contrastive & E2E Tool Calling \\
Steps / Epochs & & 3,000 steps & 2,000 steps & 3 epochs \\
Learning rate & & $2 \times 10^{-4}$ & $1 \times 10^{-4}$ & $1 \times 10^{-4}$ \\
Batch size & & 8 & 8 & 4 ($\times$4 grad accum) \\
Warmup & & 100 steps & 100 steps & 5\% of steps \\
Max grad norm & & 1.0 & 1.0 & 1.0 \\
Precision & & bf16 & bf16 & bf16 \\
Grad.\ checkpointing & & Yes & Yes & Yes \\
\midrule
Contrastive temp.\ $\tau$ & & --- & 0.07 & 0.07 \\
Hard negatives & & --- & 7 & 7 \\
AE aux.\ loss $\lambda$ & & --- & 0.1 & 0.1 (5\% freq) \\
Contrastive aux.\ $\lambda$ & & --- & --- & 0.1 \\
\midrule
Params unfrozen & & All & All & All \\
Tools per example & & 1 & 1 & 20 \\
Max schema tokens & & 256 & 256 & 256 \\
Max sequence length & & 2,048 & 2,048 & 2,048 \\
\bottomrule
\end{tabular}
\end{table}

\section{Architecture Details}
\label{app:architecture}

\begin{table}[h]
\centering
\caption{ToolFormerMicro architecture specification.}
\label{tab:arch}
\small
\begin{tabular}{ll}
\toprule
Component & Value \\
\midrule
Base model & Qwen2.5-0.5B \\
Vocab size & 151,936 \\
Hidden dimension & 896 \\
Intermediate (FFN) & 4,864 \\
Attention heads & 14 (2 KV heads, GQA) \\
Head dimension & 64 \\
Encoder layers & 6 (Qwen layers 0--5) \\
Decoder layers & 12 (Qwen layers 6--17) \\
Gist queries (K) & 8 \\
RoPE $\theta$ & 1,000,000 \\
RMSNorm $\epsilon$ & $10^{-6}$ \\
Total params & ${\sim}428$M \\
Model size (fp16) & ${\sim}857$\,MB \\
Gist cache per tool & 14\,KB \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
